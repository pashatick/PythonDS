{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pn\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pn.DataFrame(dts['data'], columns = dts['feature_names'])\n",
    "y = pn.DataFrame(dts['target'], columns = ['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7112260057484878"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Показатель R2 близок к 1, значит модель работает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=12,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=1000, max_depth=12, random_state=42)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavel/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=12,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=None,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predf = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8749965273218174"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, predf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'warn'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_samples_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_leaf_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_impurity_decrease\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_impurity_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbootstrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moob_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "A random forest regressor.\n",
       "\n",
       "A random forest is a meta estimator that fits a number of classifying\n",
       "decision trees on various sub-samples of the dataset and uses averaging\n",
       "to improve the predictive accuracy and control over-fitting.\n",
       "The sub-sample size is always the same as the original\n",
       "input sample size but the samples are drawn with replacement if\n",
       "`bootstrap=True` (default).\n",
       "\n",
       "Read more in the :ref:`User Guide <forest>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_estimators : integer, optional (default=10)\n",
       "    The number of trees in the forest.\n",
       "\n",
       "    .. versionchanged:: 0.20\n",
       "       The default value of ``n_estimators`` will change from 10 in\n",
       "       version 0.20 to 100 in version 0.22.\n",
       "\n",
       "criterion : string, optional (default=\"mse\")\n",
       "    The function to measure the quality of a split. Supported criteria\n",
       "    are \"mse\" for the mean squared error, which is equal to variance\n",
       "    reduction as feature selection criterion, and \"mae\" for the mean\n",
       "    absolute error.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "       Mean Absolute Error (MAE) criterion.\n",
       "\n",
       "max_depth : integer or None, optional (default=None)\n",
       "    The maximum depth of the tree. If None, then nodes are expanded until\n",
       "    all leaves are pure or until all leaves contain less than\n",
       "    min_samples_split samples.\n",
       "\n",
       "min_samples_split : int, float, optional (default=2)\n",
       "    The minimum number of samples required to split an internal node:\n",
       "\n",
       "    - If int, then consider `min_samples_split` as the minimum number.\n",
       "    - If float, then `min_samples_split` is a fraction and\n",
       "      `ceil(min_samples_split * n_samples)` are the minimum\n",
       "      number of samples for each split.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_samples_leaf : int, float, optional (default=1)\n",
       "    The minimum number of samples required to be at a leaf node.\n",
       "    A split point at any depth will only be considered if it leaves at\n",
       "    least ``min_samples_leaf`` training samples in each of the left and\n",
       "    right branches.  This may have the effect of smoothing the model,\n",
       "    especially in regression.\n",
       "\n",
       "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
       "    - If float, then `min_samples_leaf` is a fraction and\n",
       "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
       "      number of samples for each node.\n",
       "\n",
       "    .. versionchanged:: 0.18\n",
       "       Added float values for fractions.\n",
       "\n",
       "min_weight_fraction_leaf : float, optional (default=0.)\n",
       "    The minimum weighted fraction of the sum total of weights (of all\n",
       "    the input samples) required to be at a leaf node. Samples have\n",
       "    equal weight when sample_weight is not provided.\n",
       "\n",
       "max_features : int, float, string or None, optional (default=\"auto\")\n",
       "    The number of features to consider when looking for the best split:\n",
       "\n",
       "    - If int, then consider `max_features` features at each split.\n",
       "    - If float, then `max_features` is a fraction and\n",
       "      `int(max_features * n_features)` features are considered at each\n",
       "      split.\n",
       "    - If \"auto\", then `max_features=n_features`.\n",
       "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
       "    - If \"log2\", then `max_features=log2(n_features)`.\n",
       "    - If None, then `max_features=n_features`.\n",
       "\n",
       "    Note: the search for a split does not stop until at least one\n",
       "    valid partition of the node samples is found, even if it requires to\n",
       "    effectively inspect more than ``max_features`` features.\n",
       "\n",
       "max_leaf_nodes : int or None, optional (default=None)\n",
       "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
       "    Best nodes are defined as relative reduction in impurity.\n",
       "    If None then unlimited number of leaf nodes.\n",
       "\n",
       "min_impurity_decrease : float, optional (default=0.)\n",
       "    A node will be split if this split induces a decrease of the impurity\n",
       "    greater than or equal to this value.\n",
       "\n",
       "    The weighted impurity decrease equation is the following::\n",
       "\n",
       "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
       "                            - N_t_L / N_t * left_impurity)\n",
       "\n",
       "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
       "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
       "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
       "\n",
       "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
       "    if ``sample_weight`` is passed.\n",
       "\n",
       "    .. versionadded:: 0.19\n",
       "\n",
       "min_impurity_split : float, (default=1e-7)\n",
       "    Threshold for early stopping in tree growth. A node will split\n",
       "    if its impurity is above the threshold, otherwise it is a leaf.\n",
       "\n",
       "    .. deprecated:: 0.19\n",
       "       ``min_impurity_split`` has been deprecated in favor of\n",
       "       ``min_impurity_decrease`` in 0.19. The default value of\n",
       "       ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n",
       "       will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
       "\n",
       "bootstrap : boolean, optional (default=True)\n",
       "    Whether bootstrap samples are used when building trees. If False, the\n",
       "    whole datset is used to build each tree.\n",
       "\n",
       "oob_score : bool, optional (default=False)\n",
       "    whether to use out-of-bag samples to estimate\n",
       "    the R^2 on unseen data.\n",
       "\n",
       "n_jobs : int or None, optional (default=None)\n",
       "    The number of jobs to run in parallel for both `fit` and `predict`.\n",
       "    `None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "\n",
       "random_state : int, RandomState instance or None, optional (default=None)\n",
       "    If int, random_state is the seed used by the random number generator;\n",
       "    If RandomState instance, random_state is the random number generator;\n",
       "    If None, the random number generator is the RandomState instance used\n",
       "    by `np.random`.\n",
       "\n",
       "verbose : int, optional (default=0)\n",
       "    Controls the verbosity when fitting and predicting.\n",
       "\n",
       "warm_start : bool, optional (default=False)\n",
       "    When set to ``True``, reuse the solution of the previous call to fit\n",
       "    and add more estimators to the ensemble, otherwise, just fit a whole\n",
       "    new forest. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "estimators_ : list of DecisionTreeRegressor\n",
       "    The collection of fitted sub-estimators.\n",
       "\n",
       "feature_importances_ : array of shape = [n_features]\n",
       "    The feature importances (the higher, the more important the feature).\n",
       "\n",
       "n_features_ : int\n",
       "    The number of features when ``fit`` is performed.\n",
       "\n",
       "n_outputs_ : int\n",
       "    The number of outputs when ``fit`` is performed.\n",
       "\n",
       "oob_score_ : float\n",
       "    Score of the training dataset obtained using an out-of-bag estimate.\n",
       "\n",
       "oob_prediction_ : array of shape = [n_samples]\n",
       "    Prediction computed with out-of-bag estimate on the training set.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.ensemble import RandomForestRegressor\n",
       ">>> from sklearn.datasets import make_regression\n",
       "\n",
       ">>> X, y = make_regression(n_features=4, n_informative=2,\n",
       "...                        random_state=0, shuffle=False)\n",
       ">>> regr = RandomForestRegressor(max_depth=2, random_state=0,\n",
       "...                              n_estimators=100)\n",
       ">>> regr.fit(X, y)\n",
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
       ">>> print(regr.feature_importances_)\n",
       "[0.18146984 0.81473937 0.00145312 0.00233767]\n",
       ">>> print(regr.predict([[0, 0, 0, 0]]))\n",
       "[-8.32987858]\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The default values for the parameters controlling the size of the trees\n",
       "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
       "unpruned trees which can potentially be very large on some data sets. To\n",
       "reduce memory consumption, the complexity and size of the trees should be\n",
       "controlled by setting those parameter values.\n",
       "\n",
       "The features are always randomly permuted at each split. Therefore,\n",
       "the best found split may vary, even with the same training data,\n",
       "``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
       "of the criterion is identical for several splits enumerated during the\n",
       "search of the best split. To obtain a deterministic behaviour during\n",
       "fitting, ``random_state`` has to be fixed.\n",
       "\n",
       "The default value ``max_features=\"auto\"`` uses ``n_features`` \n",
       "rather than ``n_features / 3``. The latter was originally suggested in\n",
       "[1], whereas the former was more recently justified empirically in [2].\n",
       "\n",
       "References\n",
       "----------\n",
       "\n",
       ".. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
       "\n",
       ".. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized \n",
       "       trees\", Machine Learning, 63(1), 3-42, 2006.\n",
       "\n",
       "See also\n",
       "--------\n",
       "DecisionTreeRegressor, ExtraTreesRegressor\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RandomForestRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03211748, 0.00154999, 0.0070941 , 0.0011488 , 0.01436832,\n",
       "       0.40270459, 0.01424477, 0.06403265, 0.00496762, 0.01169177,\n",
       "       0.01808961, 0.0123114 , 0.41567892])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999994"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_importances_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.998273\n",
       "1    0.001727\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pn.DataFrame(pn.read_csv('creditcard.csv'))\n",
    "df['Class'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of             Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "5            2.0  -0.425966   0.960523  1.141109 -0.168252  0.420987   \n",
       "6            4.0   1.229658   0.141004  0.045371  1.202613  0.191881   \n",
       "7            7.0  -0.644269   1.417964  1.074380 -0.492199  0.948934   \n",
       "8            7.0  -0.894286   0.286157 -0.113192 -0.271526  2.669599   \n",
       "9            9.0  -0.338262   1.119593  1.044367 -0.222187  0.499361   \n",
       "10          10.0   1.449044  -1.176339  0.913860 -1.375667 -1.971383   \n",
       "11          10.0   0.384978   0.616109 -0.874300 -0.094019  2.924584   \n",
       "12          10.0   1.249999  -1.221637  0.383930 -1.234899 -1.485419   \n",
       "13          11.0   1.069374   0.287722  0.828613  2.712520 -0.178398   \n",
       "14          12.0  -2.791855  -0.327771  1.641750  1.767473 -0.136588   \n",
       "15          12.0  -0.752417   0.345485  2.057323 -1.468643 -1.158394   \n",
       "16          12.0   1.103215  -0.040296  1.267332  1.289091 -0.735997   \n",
       "17          13.0  -0.436905   0.918966  0.924591 -0.727219  0.915679   \n",
       "18          14.0  -5.401258  -5.450148  1.186305  1.736239  3.049106   \n",
       "19          15.0   1.492936  -1.029346  0.454795 -1.438026 -1.555434   \n",
       "20          16.0   0.694885  -1.361819  1.029221  0.834159 -1.191209   \n",
       "21          17.0   0.962496   0.328461 -0.171479  2.109204  1.129566   \n",
       "22          18.0   1.166616   0.502120 -0.067300  2.261569  0.428804   \n",
       "23          18.0   0.247491   0.277666  1.185471 -0.092603 -1.314394   \n",
       "24          22.0  -1.946525  -0.044901 -0.405570 -1.013057  2.941968   \n",
       "25          22.0  -2.074295  -0.121482  1.322021  0.410008  0.295198   \n",
       "26          23.0   1.173285   0.353498  0.283905  1.133563 -0.172577   \n",
       "27          23.0   1.322707  -0.174041  0.434555  0.576038 -0.836758   \n",
       "28          23.0  -0.414289   0.905437  1.727453  1.473471  0.007443   \n",
       "29          23.0   1.059387  -0.175319  1.266130  1.186110 -0.786002   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284777  172764.0   2.079137  -0.028723 -1.343392  0.358000 -0.045791   \n",
       "284778  172764.0  -0.764523   0.588379 -0.907599 -0.418847  0.901528   \n",
       "284779  172766.0   1.975178  -0.616244 -2.628295 -0.406246  2.327804   \n",
       "284780  172766.0  -1.727503   1.108356  2.219561  1.148583 -0.884199   \n",
       "284781  172766.0  -1.139015  -0.155510  1.894478 -1.138957  1.451777   \n",
       "284782  172767.0  -0.268061   2.540315 -1.400915  4.846661  0.639105   \n",
       "284783  172768.0  -1.796092   1.929178 -2.828417 -1.689844  2.199572   \n",
       "284784  172768.0  -0.669662   0.923769 -1.543167 -1.560729  2.833960   \n",
       "284785  172768.0   0.032887   0.545338 -1.185844 -1.729828  2.932315   \n",
       "284786  172768.0  -2.076175   2.142238 -2.522704 -1.888063  1.982785   \n",
       "284787  172769.0  -1.029719  -1.110670 -0.636179 -0.840816  2.424360   \n",
       "284788  172770.0   2.007418  -0.280235 -0.208113  0.335261 -0.715798   \n",
       "284789  172770.0  -0.446951   1.302212 -0.168583  0.981577  0.578957   \n",
       "284790  172771.0  -0.515513   0.971950 -1.014580 -0.677037  0.912430   \n",
       "284791  172774.0  -0.863506   0.874701  0.420358 -0.530365  0.356561   \n",
       "284792  172774.0  -0.724123   1.485216 -1.132218 -0.607190  0.709499   \n",
       "284793  172775.0   1.971002  -0.699067 -1.697541 -0.617643  1.718797   \n",
       "284794  172777.0  -1.266580  -0.400461  0.956221 -0.723919  1.531993   \n",
       "284795  172778.0 -12.516732  10.187818 -8.476671 -2.510473 -4.586669   \n",
       "284796  172780.0   1.884849  -0.143540 -0.999943  1.506772 -0.035300   \n",
       "284797  172782.0  -0.241923   0.712247  0.399806 -0.463406  0.244531   \n",
       "284798  172782.0   0.219529   0.881246 -0.635891  0.960928 -0.152971   \n",
       "284799  172783.0  -1.775135  -0.004235  1.189786  0.331096  1.196063   \n",
       "284800  172784.0   2.039560  -0.175233 -1.196825  0.234580 -0.008713   \n",
       "284801  172785.0   0.120316   0.931005 -0.546012 -0.745097  1.130314   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9       V10       V11       V12  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  0.090794 -0.551600 -0.617801   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425 -0.166974  1.612727  1.065235   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  0.207643  0.624501  0.066084   \n",
       "3       1.247203  0.237609  0.377436 -1.387024 -0.054952 -0.226487  0.178228   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  0.753074 -0.822843  0.538196   \n",
       "5      -0.029728  0.476201  0.260314 -0.568671 -0.371407  1.341262  0.359894   \n",
       "6       0.272708 -0.005159  0.081213  0.464960 -0.099254 -1.416907 -0.153826   \n",
       "7       0.428118  1.120631 -3.807864  0.615375  1.249376 -0.619468  0.291474   \n",
       "8       3.721818  0.370145  0.851084 -0.392048 -0.410430 -0.705117 -0.110452   \n",
       "9      -0.246761  0.651583  0.069539 -0.736727 -0.366846  1.017614  0.836390   \n",
       "10     -0.629152 -1.423236  0.048456 -1.720408  1.626659  1.199644 -0.671440   \n",
       "11      3.317027  0.470455  0.538247 -0.558895  0.309755 -0.259116 -0.326143   \n",
       "12     -0.753230 -0.689405 -0.227487 -2.094011  1.323729  0.227666 -0.242682   \n",
       "13      0.337544 -0.096717  0.115982 -0.221083  0.460230 -0.773657  0.323387   \n",
       "14      0.807596 -0.422911 -1.907107  0.755713  1.151087  0.844555  0.792944   \n",
       "15     -0.077850 -0.608581  0.003603 -0.436167  0.747731 -0.793981 -0.770407   \n",
       "16      0.288069 -0.586057  0.189380  0.782333 -0.267975 -0.450311  0.936708   \n",
       "17     -0.127867  0.707642  0.087962 -0.665271 -0.737980  0.324098  0.277192   \n",
       "18     -1.763406 -1.559738  0.160842  1.233090  0.345173  0.917230  0.970117   \n",
       "19     -0.720961 -1.080664 -0.053127 -1.978682  1.638076  1.077542 -0.632047   \n",
       "20      1.309109 -0.878586  0.445290 -0.446196  0.568521  1.019151  1.298329   \n",
       "21      1.696038  0.107712  0.521502 -1.191311  0.724396  1.690330  0.406774   \n",
       "22      0.089474  0.241147  0.138082 -0.989162  0.922175  0.744786 -0.531377   \n",
       "23     -0.150116 -0.946365 -1.617935  1.544071 -0.829881 -0.583200  0.524933   \n",
       "24      2.955053 -0.063063  0.855546  0.049967  0.573743 -0.081257 -0.215745   \n",
       "25     -0.959537  0.543985 -0.104627  0.475664  0.149451 -0.856566 -0.180523   \n",
       "26     -0.916054  0.369025 -0.327260 -0.246651 -0.046139 -0.143419  0.979350   \n",
       "27     -0.831083 -0.264905 -0.220982 -1.071425  0.868559 -0.641506 -0.111316   \n",
       "28     -0.200331  0.740228 -0.029247 -0.593392 -0.346188 -0.012142  0.786796   \n",
       "29      0.578435 -0.767084  0.401046  0.699500 -0.064738  1.048292  1.005618   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284777 -1.345452  0.227476 -0.378355  0.665911  0.028351 -0.822911  0.049716   \n",
       "284778 -0.760802  0.758545  0.414698 -0.730854 -1.245088  0.874312  0.162691   \n",
       "284779  3.664740 -0.533297  0.842937  1.128798 -0.220744 -0.590756  0.654596   \n",
       "284780  0.793083 -0.527298  0.866429  0.853819 -0.195152 -1.296770  0.725295   \n",
       "284781  0.093598  0.191353  0.092211 -0.062621 -0.792066  0.659941  0.995402   \n",
       "284782  0.186479 -0.045911  0.936448 -2.419986  0.525012  1.045386  0.124585   \n",
       "284783  3.123732 -0.270714  1.657495  0.465804  0.832931 -0.344690  0.247153   \n",
       "284784  3.240843  0.181576  1.282746 -0.893890 -1.453432  0.187488 -0.390794   \n",
       "284785  3.401529  0.337434  0.925377 -0.165663 -0.386953 -0.199626  0.032017   \n",
       "284786  3.732950 -1.217430 -0.536644  0.272867  0.300342 -0.451656  0.566368   \n",
       "284787 -2.956733  0.283610 -0.332656 -0.247488 -0.328271 -1.089397 -0.694904   \n",
       "284788 -0.751373 -0.458972 -0.140140  0.959971 -0.028284 -0.635200  0.869261   \n",
       "284789 -0.605641  1.253430 -1.042610 -0.417116  0.076605 -1.291228 -0.690868   \n",
       "284790 -0.316187  0.396137  0.532364 -0.224606 -0.753365  0.362990  0.110499   \n",
       "284791 -1.046238  0.757051  0.230473 -0.506856 -1.032990 -1.187546  0.055871   \n",
       "284792 -0.482638  0.548393  0.343003 -0.226323 -0.348134 -1.381624  0.617933   \n",
       "284793  3.911336 -1.259306  1.056209  1.315006 -0.146827 -0.222959  0.496509   \n",
       "284794 -1.788600  0.314741  0.004704  0.013857 -0.815911 -1.311976 -0.946753   \n",
       "284795 -1.394465 -3.632516  5.498583  4.893089  8.655320 -1.052365  2.834865   \n",
       "284796 -0.613638  0.190241 -0.249058  0.666458  0.120908 -1.134176  0.677729   \n",
       "284797 -1.343668  0.929369 -0.206210  0.106234 -0.284708 -0.612982 -0.066655   \n",
       "284798 -1.014307  0.427126  0.121340 -0.285670 -0.111640 -1.109232 -0.453235   \n",
       "284799  5.519980 -1.518185  2.080825  1.159498 -0.594242 -1.264072  0.453596   \n",
       "284800 -0.726571  0.017050 -0.118228  0.435402  0.267772  0.523316  0.559047   \n",
       "284801 -0.235973  0.812722  0.115093 -0.204064 -0.657422  0.644837  0.190916   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  4.356170 -1.593105  2.711941   \n",
       "284803  1.058415  0.024330  0.294869  0.584800 -0.975926 -0.150189  0.915802   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454 -0.484782  0.411614  0.063119   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087 -0.399126 -1.933849 -0.962886   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180 -0.915427 -1.040458 -0.031513   \n",
       "\n",
       "             V13       V14       V15       V16       V17       V18       V19  \\\n",
       "0      -0.991390 -0.311169  1.468177 -0.470401  0.207971  0.025791  0.403993   \n",
       "1       0.489095 -0.143772  0.635558  0.463917 -0.114805 -0.183361 -0.145783   \n",
       "2       0.717293 -0.165946  2.345865 -2.890083  1.109969 -0.121359 -2.261857   \n",
       "3       0.507757 -0.287924 -0.631418 -1.059647 -0.684093  1.965775 -1.232622   \n",
       "4       1.345852 -1.119670  0.175121 -0.451449 -0.237033 -0.038195  0.803487   \n",
       "5      -0.358091 -0.137134  0.517617  0.401726 -0.058133  0.068653 -0.033194   \n",
       "6      -0.751063  0.167372  0.050144 -0.443587  0.002821 -0.611987 -0.045575   \n",
       "7       1.757964 -1.323865  0.686133 -0.076127 -1.222127 -0.358222  0.324505   \n",
       "8      -0.286254  0.074355 -0.328783 -0.210077 -0.499768  0.118765  0.570328   \n",
       "9       1.006844 -0.443523  0.150219  0.739453 -0.540980  0.476677  0.451773   \n",
       "10     -0.513947 -0.095045  0.230930  0.031967  0.253415  0.854344 -0.221365   \n",
       "11     -0.090047  0.362832  0.928904 -0.129487 -0.809979  0.359985  0.707664   \n",
       "12      1.205417 -0.317631  0.725675 -0.815612  0.873936 -0.847789 -0.683193   \n",
       "13     -0.011076 -0.178485 -0.655564 -0.199925  0.124005 -0.980496 -0.982916   \n",
       "14      0.370448 -0.734975  0.406796 -0.303058 -0.155869  0.778265  2.221868   \n",
       "15      1.047627 -1.066604  1.106953  1.660114 -0.279265 -0.419994  0.432535   \n",
       "16      0.708380 -0.468647  0.354574 -0.246635 -0.009212 -0.595912 -0.575682   \n",
       "17      0.252624 -0.291896 -0.184520  1.143174 -0.928709  0.680470  0.025436   \n",
       "18     -0.266568 -0.479130 -0.526609  0.472004 -0.725481  0.075081 -0.406867   \n",
       "19     -0.416957  0.052011 -0.042979 -0.166432  0.304241  0.554432  0.054230   \n",
       "20      0.420480 -0.372651 -0.807980 -2.044557  0.515663  0.625847 -1.300408   \n",
       "21     -0.936421  0.983739  0.710911 -0.602232  0.402484 -1.737162 -2.027612   \n",
       "22     -2.105346  1.126870  0.003075  0.424425 -0.454475 -0.098871 -0.816597   \n",
       "23     -0.453375  0.081393  1.555204 -1.396895  0.783131  0.436621  2.177807   \n",
       "24      0.044161  0.033898  1.190718  0.578843 -0.975667  0.044063  0.488603   \n",
       "25     -0.655233 -0.279797 -0.211668 -0.333321  0.010751 -0.488473  0.505751   \n",
       "26      1.492285  0.101418  0.761478 -0.014584 -0.511640 -0.325056 -0.390934   \n",
       "27      0.361485  0.171945  0.782167 -1.355871 -0.216935  1.271765 -1.240622   \n",
       "28      0.635954 -0.086324  0.076804 -1.405919  0.775592 -0.942889  0.543969   \n",
       "29     -0.542002 -0.039915 -0.218683  0.004476 -0.193554  0.042388 -0.277834   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284777 -0.352007  0.569087  0.791917 -0.178713 -0.500821  0.007475 -0.205470   \n",
       "284778 -1.011819 -0.317789 -0.887823  0.482847  0.681412  0.486991 -0.022064   \n",
       "284779 -0.479779  0.120626 -0.907042 -1.186261  0.146953 -0.681104  0.295402   \n",
       "284780  0.454177 -0.821823 -0.713585 -0.611659  0.457890  0.092557  0.757367   \n",
       "284781  0.861816 -0.300746 -0.188434  0.267385 -0.991270  0.290285  0.546517   \n",
       "284782  0.572344 -1.820716 -0.138285  1.100173  2.322135  1.349583  1.091523   \n",
       "284783 -0.119068  0.849765  0.903389 -0.398816 -0.302906  0.036545 -0.202990   \n",
       "284784 -0.289171 -0.510320  0.955637  0.553781  0.567862  0.409517 -0.671301   \n",
       "284785 -0.374731  0.354051  0.041228 -0.154750 -0.482455 -0.842462 -0.206385   \n",
       "284786 -0.317804  0.855742 -0.041047  0.046620  0.017822 -0.772916 -0.354163   \n",
       "284787 -1.315146  0.946281 -0.073008 -0.307347 -0.274712 -0.487247 -0.014378   \n",
       "284788  0.996596 -0.280317  0.436079  0.397439 -0.557940 -0.536840 -0.082349   \n",
       "284789 -1.481724  0.753473 -0.191141 -1.129904  0.319074 -0.201862  0.744416   \n",
       "284790 -0.853503 -0.383112 -0.774650  0.764540  0.216671  0.488508  0.180495   \n",
       "284791 -0.352509  0.550972 -0.551838 -0.229314  0.004179 -0.730361  0.077652   \n",
       "284792  1.428297  0.494490  0.426827 -0.229599 -0.494065  0.176169  0.182152   \n",
       "284793 -0.051785 -0.000744  0.893684  0.000363 -0.583710  0.074163 -0.356789   \n",
       "284794 -2.119626  0.609469 -0.348707  0.077871 -0.357628 -0.578977 -0.518738   \n",
       "284795  1.088141  1.288401 -0.931503  1.067864  0.586179 -0.085524 -0.571605   \n",
       "284796  0.345928  0.002019 -0.679626 -0.549982 -0.199950 -0.420551 -0.283278   \n",
       "284797 -0.732987  0.237948 -0.293959 -0.245496 -0.174846 -0.891169 -0.290491   \n",
       "284798 -1.046946  1.122674  1.243518 -1.431897  0.939328 -0.002373  2.894952   \n",
       "284799 -0.243142 -0.858719 -0.766554 -0.644646  0.447184  0.388721  0.792135   \n",
       "284800 -0.834660  0.626211 -0.541494  0.225361 -0.605252 -0.163049  0.561695   \n",
       "284801 -0.546329 -0.731707 -0.808036  0.599628  0.070441  0.373110  0.128904   \n",
       "284802 -0.689256  4.626942 -0.924459  1.107641  1.991691  0.510632 -0.682920   \n",
       "284803  1.214756 -0.675143  1.164931 -0.711757 -0.025693 -1.221179 -1.545556   \n",
       "284804 -0.183699 -0.510602  1.329284  0.140716  0.313502  0.395652 -0.577252   \n",
       "284805 -1.042082  0.449624  1.962563 -0.608577  0.509928  1.113981  2.897849   \n",
       "284806 -0.188093 -0.084316  0.041333 -0.302620 -0.660377  0.167430 -0.256117   \n",
       "\n",
       "             V20       V21       V22       V23       V24       V25       V26  \\\n",
       "0       0.251412 -0.018307  0.277838 -0.110474  0.066928  0.128539 -0.189115   \n",
       "1      -0.069083 -0.225775 -0.638672  0.101288 -0.339846  0.167170  0.125895   \n",
       "2       0.524980  0.247998  0.771679  0.909412 -0.689281 -0.327642 -0.139097   \n",
       "3      -0.208038 -0.108300  0.005274 -0.190321 -1.175575  0.647376 -0.221929   \n",
       "4       0.408542 -0.009431  0.798278 -0.137458  0.141267 -0.206010  0.502292   \n",
       "5       0.084968 -0.208254 -0.559825 -0.026398 -0.371427 -0.232794  0.105915   \n",
       "6      -0.219633 -0.167716 -0.270710 -0.154104 -0.780055  0.750137 -0.257237   \n",
       "7      -0.156742  1.943465 -1.015455  0.057504 -0.649709 -0.415267 -0.051634   \n",
       "8       0.052736 -0.073425 -0.268092 -0.204233  1.011592  0.373205 -0.384157   \n",
       "9       0.203711 -0.246914 -0.633753 -0.120794 -0.385050 -0.069733  0.094199   \n",
       "10     -0.387226 -0.009302  0.313894  0.027740  0.500512  0.251367 -0.129478   \n",
       "11      0.125992  0.049924  0.238422  0.009130  0.996710 -0.767315 -0.492208   \n",
       "12     -0.102756 -0.231809 -0.483285  0.084668  0.392831  0.161135 -0.354990   \n",
       "13     -0.153197 -0.036876  0.074412 -0.071407  0.104744  0.548265  0.104094   \n",
       "14     -1.582122  1.151663  0.222182  1.020586  0.028317 -0.232746 -0.235557   \n",
       "15      0.263451  0.499625  1.353650 -0.256573 -0.065084 -0.039124 -0.087086   \n",
       "16     -0.113910 -0.024612  0.196002  0.013802  0.103758  0.364298 -0.382261   \n",
       "17     -0.047021 -0.194796 -0.672638 -0.156858 -0.888386 -0.342413 -0.049027   \n",
       "18     -2.196848 -0.503600  0.984460  2.458589  0.042119 -0.481631 -0.621272   \n",
       "19     -0.387910 -0.177650 -0.175074  0.040002  0.295814  0.332931 -0.220385   \n",
       "20     -0.138334 -0.295583 -0.571955 -0.050881 -0.304215  0.072001 -0.422234   \n",
       "21     -0.269321  0.143997  0.402492 -0.048508 -1.371866  0.390814  0.199964   \n",
       "22     -0.307169  0.018702 -0.061972 -0.103855 -0.370415  0.603200  0.108556   \n",
       "23     -0.230983  1.650180  0.200454 -0.185353  0.423073  0.820591 -0.227632   \n",
       "24     -0.216715 -0.579526 -0.799229  0.870300  0.983421  0.321201  0.149650   \n",
       "25     -0.386694 -0.403639 -0.227404  0.742435  0.398535  0.249212  0.274404   \n",
       "26      0.027878  0.067003  0.227812 -0.150487  0.435045  0.724825 -0.337082   \n",
       "27     -0.522951 -0.284376 -0.323357 -0.037710  0.347151  0.559639 -0.280158   \n",
       "28      0.097308  0.077237  0.457331 -0.038500  0.642522 -0.183891 -0.277464   \n",
       "29     -0.178023  0.013676  0.213734  0.014462  0.002951  0.294638 -0.395070   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284777 -0.272447  0.235758  0.829758 -0.002063  0.001344  0.262183 -0.105327   \n",
       "284778  0.024870  0.003530 -0.431876  0.141759  0.587119 -0.200998  0.267337   \n",
       "284779 -0.168378  0.086043  0.543613 -0.032129  0.768379  0.477688 -0.031833   \n",
       "284780  0.331940 -0.094708  0.236818 -0.204280  1.158185  0.627801 -0.399981   \n",
       "284781  0.341409 -0.191027 -0.631658 -0.147249  0.212931  0.354257 -0.241068   \n",
       "284782  0.111808 -0.263889 -0.857904  0.235172 -0.681794 -0.668894  0.044657   \n",
       "284783  0.319366  0.271170  1.145750  0.084783  0.721269 -0.529906 -0.240117   \n",
       "284784  0.000965  0.183856  0.202670 -0.373023  0.651122  1.073823  0.844590   \n",
       "284785  0.022677 -0.266113 -0.716336  0.108519  0.688519 -0.460220  0.161939   \n",
       "284786 -0.308523  2.016666 -1.588269  0.588482  0.632444 -0.201064  0.199251   \n",
       "284787  0.218776  0.353722  0.488487  0.293632  0.107812 -0.935586  1.138216   \n",
       "284788 -0.143294 -0.208260 -0.430347  0.416765  0.064819 -0.608337  0.268436   \n",
       "284789 -0.203306  0.851800  0.305268 -0.148093 -0.038712  0.010209 -0.362666   \n",
       "284790 -0.177211 -0.280302 -0.849919  0.300245  0.000607 -0.376379  0.128660   \n",
       "284791 -0.162132 -0.108846 -0.480820 -0.074513 -0.003988 -0.113149  0.280378   \n",
       "284792 -0.077202  0.414621  1.307511 -0.059545  0.242669 -0.665424 -0.269869   \n",
       "284793 -0.153581  0.188758  0.694418  0.163002  0.726365 -0.058282 -0.191813   \n",
       "284794 -0.029539 -0.157831 -0.883365  0.088485 -0.076790 -0.095833  0.132720   \n",
       "284795  3.490065 -0.944759 -1.565026  0.890675 -1.253276  1.786717  0.320763   \n",
       "284796 -0.153997  0.144008  0.634646 -0.042114 -0.053206  0.316403 -0.461441   \n",
       "284797 -0.139512 -0.228876 -0.514376  0.279598  0.371441 -0.559238  0.113144   \n",
       "284798  0.006666  0.099936  0.337120  0.251791  0.057688 -1.508368  0.144023   \n",
       "284799  0.348176  0.103302  0.654850 -0.348929  0.745323  0.704545 -0.127579   \n",
       "284800 -0.256922 -0.268048 -0.717211  0.297930 -0.359769 -0.315610  0.201114   \n",
       "284801  0.000676 -0.314205 -0.808520  0.050343  0.102800 -0.435870  0.124079   \n",
       "284802  1.475829  0.213454  0.111864  1.014480 -0.509348  1.436807  0.250034   \n",
       "284803  0.059616  0.214205  0.924384  0.012463 -1.016226 -0.606624 -0.395255   \n",
       "284804  0.001396  0.232045  0.578229 -0.037501  0.640134  0.265745 -0.087371   \n",
       "284805  0.127434  0.265245  0.800049 -0.163298  0.123205 -0.569159  0.546668   \n",
       "284806  0.382948  0.261057  0.643078  0.376777  0.008797 -0.473649 -0.818267   \n",
       "\n",
       "             V27       V28  Amount  Class  \n",
       "0       0.133558 -0.021053  149.62      0  \n",
       "1      -0.008983  0.014724    2.69      0  \n",
       "2      -0.055353 -0.059752  378.66      0  \n",
       "3       0.062723  0.061458  123.50      0  \n",
       "4       0.219422  0.215153   69.99      0  \n",
       "5       0.253844  0.081080    3.67      0  \n",
       "6       0.034507  0.005168    4.99      0  \n",
       "7      -1.206921 -1.085339   40.80      0  \n",
       "8       0.011747  0.142404   93.20      0  \n",
       "9       0.246219  0.083076    3.68      0  \n",
       "10      0.042850  0.016253    7.80      0  \n",
       "11      0.042472 -0.054337    9.99      0  \n",
       "12      0.026416  0.042422  121.50      0  \n",
       "13      0.021491  0.021293   27.50      0  \n",
       "14     -0.164778 -0.030154   58.80      0  \n",
       "15     -0.180998  0.129394   15.99      0  \n",
       "16      0.092809  0.037051   12.99      0  \n",
       "17      0.079692  0.131024    0.89      0  \n",
       "18      0.392053  0.949594   46.80      0  \n",
       "19      0.022298  0.007602    5.00      0  \n",
       "20      0.086553  0.063499  231.71      0  \n",
       "21      0.016371 -0.014605   34.09      0  \n",
       "22     -0.040521 -0.011418    2.28      0  \n",
       "23      0.336634  0.250475   22.75      0  \n",
       "24      0.707519  0.014600    0.89      0  \n",
       "25      0.359969  0.243232   26.43      0  \n",
       "26      0.016368  0.030041   41.88      0  \n",
       "27      0.042335  0.028822   16.00      0  \n",
       "28      0.182687  0.152665   33.00      0  \n",
       "29      0.081461  0.024220   12.99      0  \n",
       "...          ...       ...     ...    ...  \n",
       "284777 -0.022363 -0.060283    1.00      0  \n",
       "284778 -0.152951 -0.065285   80.00      0  \n",
       "284779  0.014151 -0.066542   25.00      0  \n",
       "284780  0.510818  0.233265   30.00      0  \n",
       "284781 -0.161717 -0.149188   13.00      0  \n",
       "284782 -0.066751 -0.072447   12.82      0  \n",
       "284783  0.129126 -0.080620   11.46      0  \n",
       "284784 -0.286676 -0.187719   40.00      0  \n",
       "284785  0.265368  0.090245    1.79      0  \n",
       "284786  0.438657  0.172923    8.95      0  \n",
       "284787  0.025271  0.255347    9.99      0  \n",
       "284788 -0.028069 -0.041367    3.99      0  \n",
       "284789  0.503092  0.229921   60.50      0  \n",
       "284790 -0.015205 -0.021486    9.81      0  \n",
       "284791 -0.077310  0.023079   20.32      0  \n",
       "284792 -0.170579 -0.030692    3.99      0  \n",
       "284793  0.061858 -0.043716    4.99      0  \n",
       "284794 -0.028468  0.126494    0.89      0  \n",
       "284795  2.090712  1.232864    9.87      0  \n",
       "284796  0.018265 -0.041068   60.00      0  \n",
       "284797  0.131507  0.081265    5.49      0  \n",
       "284798  0.181205  0.215243   24.05      0  \n",
       "284799  0.454379  0.130308   79.99      0  \n",
       "284800 -0.080826 -0.075071    2.68      0  \n",
       "284801  0.217940  0.068803    2.69      0  \n",
       "284802  0.943651  0.823731    0.77      0  \n",
       "284803  0.068472 -0.053527   24.79      0  \n",
       "284804  0.004455 -0.026561   67.88      0  \n",
       "284805  0.108821  0.104533   10.00      0  \n",
       "284806 -0.002415  0.013649  217.00      0  \n",
       "\n",
       "[284807 rows x 31 columns]>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn.options.display.max_columns = 100\n",
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>-0.371407</td>\n",
       "      <td>1.341262</td>\n",
       "      <td>0.359894</td>\n",
       "      <td>-0.358091</td>\n",
       "      <td>-0.137134</td>\n",
       "      <td>0.517617</td>\n",
       "      <td>0.401726</td>\n",
       "      <td>-0.058133</td>\n",
       "      <td>0.068653</td>\n",
       "      <td>-0.033194</td>\n",
       "      <td>0.084968</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>-0.099254</td>\n",
       "      <td>-1.416907</td>\n",
       "      <td>-0.153826</td>\n",
       "      <td>-0.751063</td>\n",
       "      <td>0.167372</td>\n",
       "      <td>0.050144</td>\n",
       "      <td>-0.443587</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>-0.611987</td>\n",
       "      <td>-0.045575</td>\n",
       "      <td>-0.219633</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>1.249376</td>\n",
       "      <td>-0.619468</td>\n",
       "      <td>0.291474</td>\n",
       "      <td>1.757964</td>\n",
       "      <td>-1.323865</td>\n",
       "      <td>0.686133</td>\n",
       "      <td>-0.076127</td>\n",
       "      <td>-1.222127</td>\n",
       "      <td>-0.358222</td>\n",
       "      <td>0.324505</td>\n",
       "      <td>-0.156742</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>-0.410430</td>\n",
       "      <td>-0.705117</td>\n",
       "      <td>-0.110452</td>\n",
       "      <td>-0.286254</td>\n",
       "      <td>0.074355</td>\n",
       "      <td>-0.328783</td>\n",
       "      <td>-0.210077</td>\n",
       "      <td>-0.499768</td>\n",
       "      <td>0.118765</td>\n",
       "      <td>0.570328</td>\n",
       "      <td>0.052736</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>-0.366846</td>\n",
       "      <td>1.017614</td>\n",
       "      <td>0.836390</td>\n",
       "      <td>1.006844</td>\n",
       "      <td>-0.443523</td>\n",
       "      <td>0.150219</td>\n",
       "      <td>0.739453</td>\n",
       "      <td>-0.540980</td>\n",
       "      <td>0.476677</td>\n",
       "      <td>0.451773</td>\n",
       "      <td>0.203711</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
       "7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
       "9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "5  0.260314 -0.568671 -0.371407  1.341262  0.359894 -0.358091 -0.137134   \n",
       "6  0.081213  0.464960 -0.099254 -1.416907 -0.153826 -0.751063  0.167372   \n",
       "7 -3.807864  0.615375  1.249376 -0.619468  0.291474  1.757964 -1.323865   \n",
       "8  0.851084 -0.392048 -0.410430 -0.705117 -0.110452 -0.286254  0.074355   \n",
       "9  0.069539 -0.736727 -0.366846  1.017614  0.836390  1.006844 -0.443523   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "5  0.517617  0.401726 -0.058133  0.068653 -0.033194  0.084968 -0.208254   \n",
       "6  0.050144 -0.443587  0.002821 -0.611987 -0.045575 -0.219633 -0.167716   \n",
       "7  0.686133 -0.076127 -1.222127 -0.358222  0.324505 -0.156742  1.943465   \n",
       "8 -0.328783 -0.210077 -0.499768  0.118765  0.570328  0.052736 -0.073425   \n",
       "9  0.150219  0.739453 -0.540980  0.476677  0.451773  0.203711 -0.246914   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "5 -0.559825 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080   \n",
       "6 -0.270710 -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168   \n",
       "7 -1.015455  0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   \n",
       "8 -0.268092 -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   \n",
       "9 -0.633753 -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62      0  \n",
       "1    2.69      0  \n",
       "2  378.66      0  \n",
       "3  123.50      0  \n",
       "4   69.99      0  \n",
       "5    3.67      0  \n",
       "6    4.99      0  \n",
       "7   40.80      0  \n",
       "8   93.20      0  \n",
       "9    3.68      0  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 30)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_card = pn.DataFrame(df.drop(['Class'], axis = 1))\n",
    "X_card.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807,)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_card = pn.Series(df['Class'])\n",
    "y_card.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_card_train, X_card_test, y_card_train, y_card_test = train_test_split(X_card, y_card, test_size=0.3, random_state=100, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85443, 30)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_card_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199364, 30)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_card_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85443,)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_card_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199364,)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_card_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'n_estimators': [10, 15], \n",
    "               'max_features': np.arange(3, 5),\n",
    "               'max_depth': np.arange(4, 7)}]\n",
    "CVmodel = GridSearchCV(estimator=RandomForestClassifier(random_state=100), \n",
    "                       param_grid=parameters,\n",
    "                       scoring='roc_auc',\n",
    "                       cv=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=100, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid=[{'n_estimators': [10, 15], 'max_features': array([3, 4]), 'max_depth': array([4, 5, 6])}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CVmodel.fit(X_card_train, y_card_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 6, 'max_features': 3, 'n_estimators': 15}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CVmodel.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.98711695e-01, 1.28830484e-03],\n",
       "       [9.99737336e-01, 2.62663815e-04],\n",
       "       [9.99707135e-01, 2.92864657e-04],\n",
       "       ...,\n",
       "       [9.99717846e-01, 2.82154033e-04],\n",
       "       [9.99663013e-01, 3.36987184e-04],\n",
       "       [9.99717846e-01, 2.82154033e-04]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CVmodel.predict_proba(X_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = CVmodel.predict_proba(X_card)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0012883 , 0.00026266, 0.00029286, ..., 0.00028215, 0.00033699,\n",
       "       0.00028215])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9630902415719197"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_card, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
